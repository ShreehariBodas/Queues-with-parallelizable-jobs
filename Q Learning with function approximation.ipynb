{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54ecc696-d1b9-4830-8b4a-785cb0384345",
   "metadata": {},
   "source": [
    "## Q-Learning with function approximation - Average reward setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6851fc-33a4-499d-b268-7b4eda10e056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "from scipy.optimize import fsolve, minimize\n",
    "from scipy.integrate import quad\n",
    "from scipy import linspace, meshgrid, arange, empty, concatenate, newaxis, shape\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20308597-a876-44eb-a058-96c0bce44546",
   "metadata": {},
   "source": [
    "## Try TD-learning, SARSA\n",
    "\n",
    "### $Q(s,a; \\theta) = \\theta^{\\top} \\phi(s,a)$, where, $\\theta = [\\theta_1, \\cdots, \\theta_{10}]$ and $\\phi(s,a) = [n_1^2, \\ n_1n_2, \\ n_2^2, \\ n_1, \\ n_2, \\ a^2, \\ n_1 a, \\ n_2 a, \\ a, \\ 1]$\n",
    "## Algorithm:\n",
    "### Suppose we start in state $s_0$. Then for $t \\geq 0$,\n",
    "### $a_t = \\begin{cases} \\arg\\min_{a' \\in [0,1]} Q(s_t,a';\\theta_t) \\hspace{1 cm} \\text{w.p.} \\ 1-\\epsilon \\\\ \\text{Uniform}[0,1] \\hspace{4 cm} \\text{w.p.} \\ \\epsilon \\end{cases}$\n",
    "### $C_t, s_{t+1}$ observed\n",
    "### $\\delta_t = (C_t - \\overline{C}) + \\min_{a' \\in [0,1]} Q(s_{t+1}, a' ; \\theta) - Q(s_t, a_t ; \\theta)$\n",
    "### $\\theta_{t+1} = \\theta_t + \\alpha_t \\delta_t \\phi(s_t, a_t)$\n",
    "### $\\overline{C} \\leftarrow \\overline{C} + \\beta_t \\delta_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e903535-e1bd-48af-8def-35cd57e38c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speed_up(p, n_cores):\n",
    "    rate = 1/(1 - p*(1 - 1/max(1, n_cores)))\n",
    "    return rate\n",
    "\n",
    "# Returns transition probabilities from any state (n1, n2)\n",
    "def transition_probabilities(model_pars, state, action):\n",
    "    lam, mu, cores, p1, p2, alpha, M = model_pars\n",
    "    [n1, n2] = state\n",
    "    \n",
    "    p1 = lam*alpha if n1 < M else 0\n",
    "    p2 = lam*(1-alpha) if n2 < M else 0\n",
    "    p3 = min(n1, cores*action)*mu*speed_up(p1, cores*action/n1) if n1 > 0 else 0\n",
    "    p4 = min(n2, cores*(1-action))*mu*speed_up(p2, cores*(1-action)/n2) if n2 > 0 else 0\n",
    "    p5 = 1 - p1 - p2 - p3 - p4\n",
    "    return [p1, p2, p3, p4, p5]\n",
    "\n",
    "# Given current state, it samples the next state under the optimal core allocation policy\n",
    "# which is evaluated by solving the Bellman optimality equation\n",
    "def next_state(model_pars, current_state, action):\n",
    "    [n1, n2] = current_state\n",
    "    \n",
    "    possible_next_states = [[n1+1, n2], [n1, n2+1], [n1-1, n2], [n1, n2-1], [n1, n2]]\n",
    "    indices = [0, 1, 2, 3, 4]\n",
    "    probabilities = transition_probabilities(model_pars, current_state, action)\n",
    "    \n",
    "    next_state = possible_next_states[np.random.choice(indices, size = 1, p = probabilities)[0]]\n",
    "    return next_state\n",
    "\n",
    "def feature_vector(model_pars, state, action):\n",
    "    M = model_pars[6]\n",
    "    [n1, n2] = state\n",
    "    \n",
    "    phi = np.array([n1**2/M**2, n1*n2/M**2, n2**2/M**2, n1/M, n2/M, action**2, n1*action/M, n2*action/M, action, 1], dtype = 'object')\n",
    "    return phi\n",
    "\n",
    "def Q_approximation(model_pars, state, action, theta):\n",
    "    phi = feature_vector(model_pars, state, action)\n",
    "    return np.dot(theta, phi)\n",
    "\n",
    "def min_argmin_Q(model_pars, state, theta):\n",
    "    lam, mu, cores, p1, p2, alpha, M = model_pars\n",
    "    \n",
    "    Q_values = np.array([Q_approximation(model_pars, state, (i/cores), theta) for i in range(cores+1)])\n",
    "    argmin_Q_values = np.argmin(Q_values)\n",
    "    action = argmin_Q_values/cores\n",
    "    min_Q_value = Q_values[argmin_Q_values]\n",
    "    return [action, min_Q_value]\n",
    "\n",
    "def choose_action(model_pars, state, theta, epsilon):\n",
    "    lam, mu, cores, p1, p2, alpha, M = model_pars\n",
    "    ber = np.random.binomial(1, epsilon)\n",
    "    chosen_action = min_argmin_Q(model_pars, state, theta)[0] if ber == 0 else np.random.randint(cores + 1, size = 1)/cores\n",
    "    return chosen_action\n",
    "\n",
    "def TD_error(model_pars, current_state, next_state, action, theta, average_cost):\n",
    "    delta = (current_state[0] + current_state[1]) - average_cost \\\n",
    "            + min_argmin_Q(model_pars, next_state, theta)[1] - Q_approximation(model_pars, current_state, action, theta)\n",
    "    return delta\n",
    "\n",
    "def Q_learning(model_pars, initial_state, initial_theta, n_iters):\n",
    "    current_state = initial_state\n",
    "    theta = np.zeros(10)\n",
    "    \n",
    "    cost = 0\n",
    "    avg_cost = 0\n",
    "    delta = 0\n",
    "    \n",
    "    epsilon_iterates = []\n",
    "    immediate_cost_iterates = []\n",
    "    delta_iterates = []\n",
    "    theta_iterates = []\n",
    "    avg_cost_iterates = []\n",
    "    \n",
    "    N_0 = 0.2*n_iters\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        if i < N_0: # Explore for first N_0 steps\n",
    "            action = np.random.uniform(low = 0, high = 1, size = 1)\n",
    "            epsilon_iterates.append(1)\n",
    "        else: # After N_0 steps, follow an epsilon-greedy policy\n",
    "            # Choosing epsilon for epsilon-greedy policy\n",
    "            epsilon = 1/math.sqrt(i+1)\n",
    "            epsilon_iterates.append(epsilon)\n",
    "            # Taking action\n",
    "            action = choose_action(model_pars, current_state, theta, epsilon)\n",
    "        # Cost and next state\n",
    "        immediate_cost = current_state[0] + current_state[1]\n",
    "        immediate_cost_iterates.append(immediate_cost)\n",
    "        future_state = next_state(model_pars, current_state, action)\n",
    "        # TD error\n",
    "        delta = TD_error(model_pars, current_state, future_state, action, theta, avg_cost)\n",
    "        delta_iterates.append(delta)\n",
    "        # theta update\n",
    "        theta = theta + (1/math.pow(i+1, 0.75))*delta*feature_vector(model_pars, current_state, action)\n",
    "        theta_iterates.append(theta)\n",
    "        # average cost update\n",
    "        avg_cost = avg_cost + (1/math.pow(i+1, 0.75))*delta\n",
    "        avg_cost_iterates.append(avg_cost)\n",
    "        \n",
    "    return [epsilon_iterates, immediate_cost_iterates, delta_iterates, theta_iterates, avg_cost_iterates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e298805-c956-4d57-b0c3-82bf892dc6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "lam = 4\n",
    "mu = 1\n",
    "cores = 10\n",
    "p1 = 0.4\n",
    "p2 = 0.75\n",
    "alpha = 0.4\n",
    "M = 20\n",
    "\n",
    "scale = lam + M*mu*(speed_up(p1, cores) + speed_up(p2, cores))\n",
    "\n",
    "# Scaled model parameters\n",
    "model_pars = [lam/scale, mu/scale, cores, p1, p2, alpha, M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd6b8d4-b573-4159-b188-c833f3a93fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning\n",
    "initial_state = [0, 0]\n",
    "initial_theta = np.random.random(10) \n",
    "n_iters = 10000\n",
    "\n",
    "[epsilon_iterates, immediate_cost_iterates, delta_iterates, theta_iterates, avg_cost_iterates] = Q_learning(model_pars, initial_state, initial_theta, n_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae65fac-1764-41b3-b6ce-c591c3b62d8e",
   "metadata": {},
   "source": [
    "## Graph plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc41066a-0e3e-4d75-b81f-e5d5220446da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(delta_iterates, color=\"blue\")\n",
    "plt.xlabel(\"Iterate\")\n",
    "plt.ylabel(\"TD Error\")\n",
    "plt.title(\"TD Error iterates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a6ef6b-d7cb-4a7b-8bcf-8dcaa38d5f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
