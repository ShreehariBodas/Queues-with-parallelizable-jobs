{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33efaca0-4ebf-472d-8b33-ad6193009a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "from scipy.optimize import fsolve, minimize\n",
    "from scipy.integrate import quad\n",
    "from scipy import linspace, meshgrid, arange, empty, concatenate, newaxis, shape\n",
    "from collections import deque\n",
    "import nbimporter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5141ca01-5ba4-49b8-af6e-beecb99ad2b1",
   "metadata": {},
   "source": [
    "## MDP functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd9321e4-6a78-4565-ae83-7a3013622e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following functions solve Bellman optimality equation\n",
    "def A(model_pars, state, V):\n",
    "    # Unpacking\n",
    "    lam, mu, cores, p1, p2, alpha, M = model_pars\n",
    "    [n1, n2] = state\n",
    "    # Calculating A\n",
    "    A_val = (n1 + n2) + \\\n",
    "            (lam*alpha*(V[n1+1, n2] - V[n1, n2]) if n1 < M else 0) + \\\n",
    "            (lam*(1-alpha)*(V[n1, n2+1] - V[n1, n2]) if n2 < M else 0)\n",
    "    return A_val\n",
    "    \n",
    "    \n",
    "def H_and_action(model_pars, state, V):\n",
    "    # Unpacking\n",
    "    lam, mu, cores, p1, p2, alpha, M = model_pars\n",
    "    [n1, n2] = state\n",
    "    \n",
    "    # Evaluating best continuous action\n",
    "    # min_cost = minimize(cost, 0.5 , method=\"L-BFGS-B\", args = (model_pars, state, V), bounds = [(0,1)])\n",
    "    # a_optimal = min_cost.x\n",
    "    # H_optimal = V[n1, n2] + float(min_cost.fun)\n",
    "    \n",
    "    # Evaluating best discrete action from [0, 1/cores, 2/cores, ..., 1]\n",
    "    costs_given_action = np.array([cost(i/cores, model_pars, state, V) for i in  range(cores+1)])\n",
    "    index = np.argmin(costs_given_action)\n",
    "    a_optimal = index/cores\n",
    "    H_optimal = V[n1, n2] + costs_given_action[index]\n",
    "    return H_optimal, a_optimal\n",
    "    \n",
    "    \n",
    "def cost(action, model_pars, state, V):\n",
    "    # Unpacking\n",
    "    lam, mu, cores, p1, p2, alpha, M = model_pars\n",
    "    [n1, n2] = state\n",
    "    # Calculating cost\n",
    "    cost = (min(n1, cores*action)*mu*speed_up(p1, cores*action/n1)*(V[n1-1, n2] - V[n1, n2]) if n1 > 0 else 0) + \\\n",
    "           (min(n2, cores*(1-action))*mu*speed_up(p2, cores*(1-action)/n2)*(V[n1, n2-1] - V[n1, n2]) if n2 > 0 else 0)\n",
    "    return cost\n",
    "\n",
    "\n",
    "def solve_MDP(model_pars):\n",
    "    # Unpacking\n",
    "    lam, mu, cores, p1, p2, alpha, M = model_pars\n",
    "    # We define old and new value function and update them.\n",
    "    old_V = np.full((M+1, M+1), 1.0, dtype=np.float64)\n",
    "    new_V = np.full((M+1, M+1), 1.0, dtype=np.float64)\n",
    "    # Change in old_V and new_V should eventually converge to average cost per unit time.\n",
    "    change_V = np.zeros((M+1, M+1))\n",
    "    # Stores optimal action\n",
    "    action = np.zeros((M+1, M+1))\n",
    "    \n",
    "    epsilon = 0.01 # Maximum permissible error\n",
    "    delta = math.inf\n",
    "    \n",
    "    iteration = 1\n",
    "    \n",
    "    while delta >= epsilon:\n",
    "        fraction_change = 1\n",
    "        for i in range(M+1):\n",
    "            for j in range(M+1):\n",
    "                term_1 = A(model_pars, [i,j], old_V)\n",
    "                term_2, a = H_and_action(model_pars, [i,j], old_V)\n",
    "                action[i,j] = a\n",
    "                new_V[i,j] = term_1 + float(term_2)\n",
    "                fraction_change = max(fraction_change, (new_V[i,j] - old_V[i,j])/old_V[i,j] if old_V[i,j] > 0 else 1)\n",
    "        change_V = new_V.copy() - old_V.copy()\n",
    "        delta = change_V.max() - change_V.min()\n",
    "        # print(\"Iteration = \", iteration, \" Delta = \", delta)\n",
    "        old_V = new_V.copy()\n",
    "        iteration += 1\n",
    "        \n",
    "    return new_V, action\n",
    "\n",
    "\n",
    "def bellman_optimal_policy(pars):\n",
    "    # Unpacking\n",
    "    lam, mu, cores, p1, p2, alpha, M = pars\n",
    "    # Scales the parameters so that sum of arrival and departure rates is smaller than 1.\n",
    "    scale = lam + M*mu*(speed_up(p1, cores) + speed_up(p2, cores))\n",
    "    # scale = lam + mu*np.max([min(M, c)*speed_up(p1, c) + min(M, cores-c)*speed_up(p2, cores-c) for c in range(cores+1)])\n",
    "    model_pars = [lam/scale, mu/scale, cores, p1, p2, alpha, M]\n",
    "    # Optimal value function and optimal policy\n",
    "    V_optimal, pi_optimal  = solve_MDP(model_pars)\n",
    "    relative_V_optimal = V_optimal - V_optimal[0,0]\n",
    "    # Calculate relative Q values\n",
    "    relative_Q_optimal = np.zeros((M+1, M+1, cores+1))\n",
    "    for i in range(M+1):\n",
    "        for j in range(M+1):\n",
    "            for k in range(cores+1):\n",
    "                probs = transition_probabilities(model_pars, [i,j], k/cores)\n",
    "                future_cost = 0\n",
    "                future_cost += probs[0]*relative_V_optimal[i+1, j] if i < M else 0\n",
    "                future_cost += probs[1]*relative_V_optimal[i, j+1] if j < M else 0\n",
    "                future_cost += probs[2]*relative_V_optimal[i-1, j] if i > 0 else 0\n",
    "                future_cost += probs[3]*relative_V_optimal[i, j-1] if j > 0 else 0\n",
    "                future_cost += probs[4]*relative_V_optimal[i, j]\n",
    "                relative_Q_optimal[i, j, k] = (i+j) + future_cost\n",
    "    \n",
    "    return pi_optimal, relative_V_optimal, relative_Q_optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f412f53-7410-453f-b63a-033d22cf1a9d",
   "metadata": {},
   "source": [
    "## General functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa716226-34a1-4753-8790-0d38d063375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speed_up(p, n_cores):\n",
    "    rate = 1/(1 - p*(1 - 1/max(1, n_cores)))\n",
    "    return rate\n",
    "\n",
    "\n",
    "# Returns transition probabilities from any state (n1, n2)\n",
    "def transition_probabilities(model_pars, state, action):\n",
    "    lam, mu, cores, p1, p2, alpha, M = model_pars\n",
    "    [n1, n2] = state\n",
    "    \n",
    "    prob_1 = lam*alpha if n1 < M else 0\n",
    "    prob_2 = lam*(1-alpha) if n2 < M else 0\n",
    "    prob_3 = min(n1, cores*action)*mu*speed_up(p1, cores*action/n1) if n1 > 0 else 0\n",
    "    prob_4 = min(n2, cores*(1-action))*mu*speed_up(p2, cores*(1-action)/n2) if n2 > 0 else 0\n",
    "    prob_5 = 1 - prob_1 - prob_2 - prob_3 - prob_4\n",
    "    return [prob_1, prob_2, prob_3, prob_4, prob_5]\n",
    "\n",
    "\n",
    "# Given current state, it samples the next state under the optimal core allocation policy\n",
    "# which is evaluated by solving the Bellman optimality equation\n",
    "def sample_next_state(model_pars, current_state, action):\n",
    "    [n1, n2] = current_state\n",
    "    \n",
    "    possible_next_states = [[n1+1, n2], [n1, n2+1], [n1-1, n2], [n1, n2-1], [n1, n2]]\n",
    "    indices = [0, 1, 2, 3, 4]\n",
    "    probabilities = transition_probabilities(model_pars, current_state, action)\n",
    "    next_state = np.array(possible_next_states[np.random.choice(indices, size = 1, p = probabilities)[0]])\n",
    "    return next_state\n",
    "\n",
    "\n",
    "# Heatmap of two policies, and their difference.\n",
    "def visualize_policies(policy_1, policy_2):\n",
    "    policy_diff = policy_1 - policy_2\n",
    "    \n",
    "    # Policy 1 heatmap\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    sns.heatmap(policy_1, ax=axes[0], cmap=\"coolwarm\", annot=False, cbar=True)\n",
    "    axes[0].set_title(\"Heatmap of Policy 1\")\n",
    "    axes[0].invert_yaxis()  # This makes (0,0) appear at bottom left\n",
    "    # Policy 2 heatmap\n",
    "    sns.heatmap(policy_2, ax=axes[1], cmap=\"coolwarm\", annot=False, cbar=True)\n",
    "    axes[1].set_title(\"Heatmap of Policy 2\")\n",
    "    axes[1].invert_yaxis()  # This makes (0,0) appear at bottom left\n",
    "    # Policy difference heatmap\n",
    "    sns.heatmap(policy_diff, ax=axes[2], cmap=\"coolwarm\", annot=False, cbar=True)\n",
    "    axes[2].set_title(\"Policy difference\")\n",
    "    axes[2].invert_yaxis()  # This makes (0,0) appear at bottom left\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56b07cdb-9de7-4bfd-ae4a-3c041df8e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How Q-values vary across the state space given action\n",
    "def plot_q_heatmap(Q, cores, action):\n",
    "    fig, ax = plt.subplots(figsize=(18, 15))\n",
    "    data = Q[:, :, int(cores * action)]\n",
    "    sns.heatmap(data, cmap=\"viridis\", annot=False, ax=ax)\n",
    "    # Move x-axis to top\n",
    "    ax.xaxis.set_ticks_position('top')\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    # Grid coordinate labels\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            ax.text(j + 0.5, i + 0.5, f\"{np.round(Q[i,j], 1)}\", ha='center', va='center', fontsize=5, color='white')\n",
    "    ax.set_title(f\"Q-values Heatmap for action = {action}\", pad=30)\n",
    "    ax.set_xlabel(\"$n_2$ (Good Jobs)\")\n",
    "    ax.set_ylabel(\"$n_1$ (Bad Jobs)\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# How Q-values vary with state for a fixed action\n",
    "def plot_q_surface(Q, cores, action):\n",
    "    n1_vals, n2_vals = np.meshgrid(range(Q.shape[0]), range(Q.shape[1]), indexing=\"ij\")\n",
    "    q_vals = Q[:, :, int(cores*action)]\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_surface(n1_vals, n2_vals, q_vals, cmap=\"viridis\")\n",
    "\n",
    "    ax.set_title(f\"Q-values Surface Plot for action = {action}\")\n",
    "    ax.set_xlabel(\"$n_1$ (Bad Jobs)\")\n",
    "    ax.set_ylabel(\"$n_2$ (Good Jobs)\")\n",
    "    ax.set_zlabel(\"Q-value\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# How Q-values change across actions for a fixed state (i,j)\n",
    "def plot_q_vs_actions(Q, state):\n",
    "    [n1, n2] = state\n",
    "    actions = np.arange(Q.shape[2])\n",
    "    q_values = Q[n1, n2, :]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(actions, q_values, marker='o', linestyle='-', color='b')\n",
    "    plt.title(f\"Q-values for State ({n1}, {n2})\")\n",
    "    plt.xlabel(\"Action k\")\n",
    "    plt.ylabel(\"Q-value\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e53874-fb6c-42d4-b902-7c5253e7f56c",
   "metadata": {},
   "source": [
    "## Q-Learning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "480003f9-6407-43d9-9332-50c6b344d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(model_pars, initial_state, n_iters):\n",
    "    lam, mu, cores, p1, p2, alpha, M = model_pars\n",
    "    current_state = initial_state\n",
    "    next_state = np.zeros(2)\n",
    "    \n",
    "    Q_values = np.zeros((M+1, M+1, cores+1))\n",
    "    count_state_action_visits = np.zeros((M+1, M+1, cores+1))\n",
    "    long_run_average_cost = 0\n",
    "    state_history = [current_state]\n",
    "    long_run_average_cost_history = [long_run_average_cost]\n",
    "    \n",
    "    for t in range(n_iters):\n",
    "        # Given current state, we take action\n",
    "        epsilon = 1 / math.pow((1+t), 0.5)\n",
    "        ber = np.random.binomial(1, epsilon)\n",
    "        action = np.argmin(np.array(Q_values[current_state[0], current_state[1], :]))/cores if ber == 0 else np.random.randint(low = 0, high = cores+1, size = 1)[0]/cores\n",
    "        # Count state-action frequency\n",
    "        count_state_action_visits[current_state[0], current_state[1], int(cores*action)] += 1\n",
    "        # After taking action, we observe immediate cost\n",
    "        immediate_cost = current_state[0] + current_state[1]\n",
    "        # Then, we observe future state\n",
    "        next_state = sample_next_state(model_pars, current_state, action)\n",
    "        state_history.append(next_state)\n",
    "        # Long run average cost update\n",
    "        beta = 1/math.pow((1+t),1)\n",
    "        long_run_average_cost = (1-beta)*long_run_average_cost + beta*immediate_cost\n",
    "        long_run_average_cost_history.append(long_run_average_cost)\n",
    "        # Q update\n",
    "        alpha = 1/(1 + 0.01*t)\n",
    "        Q_values[current_state[0], current_state[1], int(cores*action)] \\\n",
    "                = (1-alpha)*Q_values[current_state[0], current_state[1], int(cores*action)] \\\n",
    "                   + alpha*(immediate_cost - long_run_average_cost + np.min(np.array(Q_values[next_state[0], next_state[1], :])) - Q_values[0, 0, 0])\n",
    "        # State update\n",
    "        current_state = next_state\n",
    "    # Evaluating optimal policy as argmin of Q-values    \n",
    "    optimal_actions = np.zeros((M+1, M+1))\n",
    "    for n1 in range(M+1):\n",
    "        for n2 in range(M+1):\n",
    "            argmin_index = np.argmin(np.array(Q_values[n1, n2, :]))\n",
    "            optimal_actions[n1, n2] = argmin_index/cores\n",
    "            \n",
    "    return Q_values, optimal_actions, long_run_average_cost_history, count_state_action_visits\n",
    "\n",
    "\n",
    "def QL_optimal_policy(pars):\n",
    "    # Unpacking\n",
    "    lam, mu, cores, p1, p2, alpha, M = pars\n",
    "    # Scales the parameters so that sum of arrival and departure rates is smaller than 1.\n",
    "    # scale = lam + M*mu*(speed_up(p1, cores) + speed_up(p2, cores))\n",
    "    scale = lam + mu*np.max([min(M, c)*speed_up(p1, c) + min(M, cores-c)*speed_up(p2, cores-c) for c in range(cores+1)])\n",
    "    model_pars = [lam/scale, mu/scale, cores, p1, p2, alpha, M]\n",
    "    # Returns Q-learning optimal policy\n",
    "    initial_state = [2,2]\n",
    "    n_iters = 100000\n",
    "    QL_Q_values, QL_pi_optimal, QL_long_run_average_cost_history, QL_count_state_action_visits = Q_learning(model_pars, initial_state, n_iters)\n",
    "    \n",
    "    return QL_Q_values, QL_pi_optimal, QL_long_run_average_cost_history, QL_count_state_action_visits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a017b6b-6212-4704-a37b-8acd602158cf",
   "metadata": {},
   "source": [
    "## SARSA functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eccf5b7-4f39-4bba-a981-f9f1ec535662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARSA(model_pars, initial_state, n_iters):\n",
    "    lam, mu, cores, p1, p2, alpha, M = model_pars\n",
    "    \n",
    "    current_state = initial_state\n",
    "    next_state = np.zeros(2)\n",
    "    current_action = np.random.randint(low = 0, high = cores+1, size = 1)[0]/cores\n",
    "    next_action = 0\n",
    "    \n",
    "    Q_values = np.zeros((M+1, M+1, cores+1))\n",
    "    long_run_average_cost = 0\n",
    "    state_history = [current_state]\n",
    "    long_run_average_cost_history = [long_run_average_cost]\n",
    "    \n",
    "    for t in range(n_iters):\n",
    "        # Immediate cost\n",
    "        immediate_cost = current_state[0] + current_state[1]\n",
    "        # Long run average cost update\n",
    "        beta = 1/math.pow((1+t),1)\n",
    "        long_run_average_cost = (1-beta)*long_run_average_cost + beta*immediate_cost\n",
    "        long_run_average_cost_history.append(long_run_average_cost)\n",
    "        # Then, we observe future state\n",
    "        next_state = sample_next_state(model_pars, current_state, current_action)\n",
    "        state_history.append(next_state)\n",
    "        # Given next state, we take next action\n",
    "        epsilon = 1 / math.pow((1+t), 0.5)\n",
    "        ber = np.random.binomial(1, epsilon)\n",
    "        next_action = np.argmin(np.array(Q_values[next_state[0], next_state[1], :]))/cores if ber == 0 else np.random.randint(low = 0, high = cores+1, size = 1)[0]/cores\n",
    "        # Q update\n",
    "        alpha = 1/(1 + 0.01*t)\n",
    "        Q_values[current_state[0], current_state[1], int(cores*current_action)] \\\n",
    "                = (1-alpha)*Q_values[current_state[0], current_state[1], int(cores*current_action)] \\\n",
    "                   + alpha*(immediate_cost - long_run_average_cost + Q_values[next_state[0], next_state[1], int(cores*next_action)])\n",
    "        # State update\n",
    "        current_state = next_state\n",
    "        current_action = next_action\n",
    "        \n",
    "    # Evaluating optimal policy as argmin of Q-values    \n",
    "    optimal_actions = np.zeros((M+1, M+1))\n",
    "    for n1 in range(M+1):\n",
    "        for n2 in range(M+1):\n",
    "            argmin_index = np.argmin(np.array(Q_values[n1, n2, :]))\n",
    "            optimal_actions[n1, n2] = argmin_index/cores\n",
    "            \n",
    "    return Q_values, optimal_actions, long_run_average_cost_history\n",
    "\n",
    "\n",
    "def SARSA_optimal_policy(pars):\n",
    "    # Unpacking\n",
    "    lam, mu, cores, p1, p2, alpha, M = pars\n",
    "    # Scales the parameters so that sum of arrival and departure rates is smaller than 1.\n",
    "    # scale = lam + M*mu*(speed_up(p1, cores) + speed_up(p2, cores))\n",
    "    scale = lam + mu*np.max([min(M, c)*speed_up(p1, c) + min(M, cores-c)*speed_up(p2, cores-c) for c in range(cores+1)])\n",
    "    model_pars = [lam/scale, mu/scale, cores, p1, p2, alpha, M]\n",
    "    # Returns Q-learning optimal policy\n",
    "    initial_state = [2,2]\n",
    "    n_iters = 100000\n",
    "    SARSA_Q_values, SARSA_pi_optimal, SARSA_long_run_average_cost_history = SARSA(model_pars, initial_state, n_iters)\n",
    "    \n",
    "    return SARSA_Q_values, SARSA_pi_optimal, SARSA_long_run_average_cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bdac82-f32b-424c-be77-2af59bf2758e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5e02a8-9fcb-4392-9897-d797e5736a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
